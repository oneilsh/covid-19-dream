workflow() %>%
summary(wf)
workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = price/carat > median(price/carat)) %>%
update_role(price, carat, new_role = "not_fair")
) %>%
pull_workflow_preprocessor() ->
wf
summary(wf)
summary(wf)
?step_mutate
qq_rec <-
recipe( ~ ., data = iris) %>%
step_mutate(
bad_approach = Sepal.Width * const,
best_approach = Sepal.Width * !!const
)
const = 1.44
qq_rec <-
recipe( ~ ., data = iris) %>%
step_mutate(
bad_approach = Sepal.Width * const,
best_approach = Sepal.Width * !!const
)
summary(qq_rec)
summary(qq_rec %>% prep())
workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = price/carat > median(price/carat)) %>%
update_role(price, carat, new_role = "not_fair") %>%
prep()
) %>%
pull_workflow_preprocessor() ->
wf
summary(wf)
summary(wf)
workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = price/carat > median(price/carat), role = "outcome") %>%
update_role(price, carat, new_role = "not_fair") %>%
prep()
) %>%
pull_workflow_preprocessor() ->
wf
summary(wf)
summary(wf)
?prep
workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = price/carat > median(price/carat), role = "outcome") %>%
update_role(price, carat, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
summary()
workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = price/carat > median(price/carat), role = "outcome") %>%
update_role(price, carat, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, carat, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, carat, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data) %>%
predict(validate_data, prob = TRUE)
workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, carat, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data) %>%
predict(validate_data, type = "prob")
workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, carat, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data) %>%
predict(validate_data, type = "prob") ->
res
head(res)
model <- workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, carat, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
model %>%
predict(validate_data, type = "prob") %>%
bind_cols(validate_data) %>%
mutate(prediction_type = "validate_data") %>%
bind_rows(model %>%
predict(train_data, type = "prob") %>%
bind_cols(train_data) %>%
mutate(prediction_type = "train_data")) ->
res
head(res)
model %>%
predict(validate_data, type = "prob") %>%
bind_cols(validate_data) %>%
mutate(prediction_type = "validate_data") %>%
bind_rows(model %>%
predict(train_data, type = "prob") %>%
bind_cols(train_data) %>%
mutate(prediction_type = "train_data")) %>%
# recompute expensiveness from training threshold
mutate(expensive = price/carat > median(train_data$price/train_data$carat)) %>%
ggplot() +
geom_histogram(aes(x = .pred_TRUE, fill = expensive)) +
facet_wrap(~ prediction_type)
model <- workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, carat, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = 20) %>%
set_engine("ranger", max.depth = 5, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
model %>%
predict(validate_data, type = "prob") %>%
bind_cols(validate_data) %>%
mutate(prediction_type = "validate_data") %>%
bind_rows(model %>%
predict(train_data, type = "prob") %>%
bind_cols(train_data) %>%
mutate(prediction_type = "train_data")) %>%
# recompute expensiveness from training threshold
mutate(expensive = price/carat > median(train_data$price/train_data$carat)) %>%
ggplot() +
geom_histogram(aes(x = .pred_TRUE, fill = expensive)) +
facet_wrap(~ prediction_type)
model %>%
predict(validate_data, type = "prob") %>%
bind_cols(validate_data) %>%
mutate(prediction_type = "validate_data") %>%
bind_rows(model %>%
predict(train_data, type = "prob") %>%
bind_cols(train_data) %>%
mutate(prediction_type = "train_data")) %>%
# recompute expensiveness from training threshold
mutate(expensive = price/carat > median(train_data$price/train_data$carat)) %>%
ggplot() +
geom_histogram(aes(x = .pred_TRUE, fill = expensive, alpha = 0.5)) +
facet_wrap(~ prediction_type)
model <- workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, carat, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
model %>%
predict(validate_data, type = "prob") %>%
bind_cols(validate_data) %>%
mutate(prediction_type = "validate_data") %>%
bind_rows(model %>%
predict(train_data, type = "prob") %>%
bind_cols(train_data) %>%
mutate(prediction_type = "train_data")) %>%
# recompute expensiveness from training threshold
mutate(expensive = price/carat > median(train_data$price/train_data$carat)) %>%
ggplot() +
geom_histogram(aes(x = .pred_TRUE, fill = expensive, alpha = 0.5)) +
facet_wrap(~ prediction_type)
model %>%
predict(validate_data, type = "prob") %>%
bind_cols(validate_data) %>%
mutate(prediction_type = "validate_data") %>%
bind_rows(model %>%
predict(train_data, type = "prob") %>%
bind_cols(train_data) %>%
mutate(prediction_type = "train_data")) %>%
# recompute expensiveness from training threshold
mutate(expensive = price/carat > median(train_data$price/train_data$carat)) %>%
ggplot() +
geom_histogram(aes(x = .pred_TRUE, fill = expensive), position = "dodge") +
facet_wrap(~ prediction_type)
?geom_histogram
model <- workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, carat, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = 20) %>%
set_engine("ranger", max.depth = 5, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
model <- workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, carat, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = 20) %>%
set_engine("ranger", max.depth = 5, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
model %>%
predict(validate_data, type = "prob") %>%
bind_cols(validate_data) %>%
mutate(prediction_type = "validate_data") %>%
bind_rows(model %>%
predict(train_data, type = "prob") %>%
bind_cols(train_data) %>%
mutate(prediction_type = "train_data")) %>%
# recompute expensiveness from training threshold
mutate(expensive = price/carat > median(train_data$price/train_data$carat)) %>%
ggplot() +
geom_histogram(aes(x = .pred_TRUE, fill = expensive), position = "dodge") +
facet_wrap(~ prediction_type)
model %>%
pull_workflow_fit() %>%
vip()
model <- workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, carat, y, x, z, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = 20) %>%
set_engine("ranger", max.depth = 5, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
model <- workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, carat, y, x, z, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = 20) %>%
set_engine("ranger", max.depth = 5, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
model %>%
predict(validate_data, type = "prob") %>%
bind_cols(validate_data) %>%
mutate(prediction_type = "validate_data") %>%
bind_rows(model %>%
predict(train_data, type = "prob") %>%
bind_cols(train_data) %>%
mutate(prediction_type = "train_data")) %>%
# recompute expensiveness from training threshold
mutate(expensive = price/carat > median(train_data$price/train_data$carat)) %>%
ggplot() +
geom_histogram(aes(x = .pred_TRUE, fill = expensive), position = "dodge") +
facet_wrap(~ prediction_type)
model <- workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, carat, y, x, z, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
model %>%
predict(validate_data, type = "prob") %>%
bind_cols(validate_data) %>%
mutate(prediction_type = "validate_data") %>%
bind_rows(model %>%
predict(train_data, type = "prob") %>%
bind_cols(train_data) %>%
mutate(prediction_type = "train_data")) %>%
# recompute expensiveness from training threshold
mutate(expensive = price/carat > median(train_data$price/train_data$carat)) %>%
ggplot() +
geom_histogram(aes(x = .pred_TRUE, fill = expensive), position = "dodge") +
facet_wrap(~ prediction_type)
model %>%
pull_workflow_fit() %>%
vip()
?diamonds
model <- workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(carat, y, x, z, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
model %>%
predict(validate_data, type = "prob") %>%
bind_cols(validate_data) %>%
mutate(prediction_type = "validate_data") %>%
bind_rows(model %>%
predict(train_data, type = "prob") %>%
bind_cols(train_data) %>%
mutate(prediction_type = "train_data")) %>%
# recompute expensiveness from training threshold
mutate(expensive = price/carat > median(train_data$price/train_data$carat)) %>%
ggplot() +
geom_histogram(aes(x = .pred_TRUE, fill = expensive), position = "dodge") +
facet_wrap(~ prediction_type)
model <- workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(carat, y, x, z, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(carat, y, x, z, new_role = "not_fair") %>%
prep()
recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(carat, y, x, z, new_role = "not_fair") %>%
prep() %>% summary()
model <- workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, new_role = "predictor") %>%
update_role(carat, y, x, z, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
model %>%
predict(validate_data, type = "prob") %>%
bind_cols(validate_data) %>%
mutate(prediction_type = "validate_data") %>%
bind_rows(model %>%
predict(train_data, type = "prob") %>%
bind_cols(train_data) %>%
mutate(prediction_type = "train_data")) %>%
# recompute expensiveness from training threshold
mutate(expensive = price/carat > median(train_data$price/train_data$carat)) %>%
ggplot() +
geom_histogram(aes(x = .pred_TRUE, fill = expensive), position = "dodge") +
facet_wrap(~ prediction_type)
model %>%
pull_workflow_fit() %>%
vip()
model <- workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, new_role = "not_fair") %>%
update_role(carat, y, x, z, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
model <- workflow() %>%
# start by pretending we'll predict price
add_recipe(recipe(price ~ ., train_data) %>%
# hmm, adding this in the recipe means a new median will be computed for each
# dataset pushed through - this is probably ok, as it will only be used
# as an outcome var during training
step_mutate(expensive = factor(price/carat > median(price/carat)), role = "outcome") %>%
update_role(price, new_role = "not_fair") %>%
update_role(carat, y, x, z, new_role = "not_fair") %>%
prep() # runs the steps that generate new data (e.g. step_mutate)
) %>%
add_model(rand_forest(trees = 100, min_n = NULL) %>%
set_engine("ranger", max.depth = NULL, importance = "permutation") %>%
set_mode("classification")) %>%
fit(train_data)
model %>%
predict(validate_data, type = "prob") %>%
bind_cols(validate_data) %>%
mutate(prediction_type = "validate_data") %>%
bind_rows(model %>%
predict(train_data, type = "prob") %>%
bind_cols(train_data) %>%
mutate(prediction_type = "train_data")) %>%
# recompute expensiveness from training threshold
mutate(expensive = price/carat > median(train_data$price/train_data$carat)) %>%
ggplot() +
geom_histogram(aes(x = .pred_TRUE, fill = expensive), position = "dodge") +
facet_wrap(~ prediction_type)
